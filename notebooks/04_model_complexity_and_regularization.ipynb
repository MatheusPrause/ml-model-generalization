{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68837b3e-36d3-48e2-a3f6-8dd944e0b578",
   "metadata": {},
   "source": [
    "# Model Complexity and Regularization\n",
    "\n",
    "In this notebook, we evaluate how increasing model complexity affects\n",
    "performance and generalization, using the baseline Linear Regression\n",
    "as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b5e611c-c995-4e5a-ba6c-32fe89998449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2278570e-6d27-411a-9ca6-eb10e2e47962",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing(as_frame=True)\n",
    "df = data.frame\n",
    "\n",
    "X = df.drop(\"MedHouseVal\", axis=1)\n",
    "y = df[\"MedHouseVal\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61dc12-588f-4ca9-b5c7-34d455eca700",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "Decision Trees can capture complex non_linear relations but are prone to overfitting if not properly constrainded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "053cbc07-02ce-41eb-9661-0146ee2c728e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(3.218325866275131e-16),\n",
       " np.float64(0.7037294974840077),\n",
       " 1.0,\n",
       " 0.622075845135081)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = DecisionTreeRegressor(random_state=42)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = tree.predict(X_train)\n",
    "y_test_pred = tree.predict(X_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "rmse_train, rmse_test, r2_train, r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf89dc60-fb06-4bed-9b64-903b5a92a7e4",
   "metadata": {},
   "source": [
    "### Fully Grown Decision Tree\n",
    "\n",
    "The unrestricted Decision Tree achieved near-perfect performance on the\n",
    "training set, with almost zero error and an R² score of 1.0.\n",
    "\n",
    "However, its performance drops substantially on the test set, indicating\n",
    "severe overfitting. The model memorizes the training data instead of\n",
    "learning generalizable patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb8239f-a137-470c-aae8-9d86dedef3ff",
   "metadata": {},
   "source": [
    "## Overfitting Analysis\n",
    "\n",
    "The decision Tree achieves very low training error but significantly worse performance on the test set. This gap indicates overfitting, as the model memorizes training instead of generalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc642b92-5749-4d4b-894e-90eebd08d1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.696225521098421),\n",
       " np.float64(0.7245672649492821),\n",
       " 0.637389379963111,\n",
       " 0.599363458161861)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_limited = DecisionTreeRegressor(\n",
    "    max_depth=5,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_limited.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = tree_limited.predict(X_train)\n",
    "y_test_pred = tree_limited.predict(X_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "rmse_train, rmse_test, r2_train, r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7d7f7f-9f49-44c8-95b9-e4c1b3f55b71",
   "metadata": {},
   "source": [
    "### Regularized Decision Tree\n",
    "\n",
    "By constraining tree depth and minimum leaf size, the model reduces\n",
    "variance and improves generalization.\n",
    "\n",
    "The smaller gap between training and test performance indicates a better\n",
    "bias–variance balance compared to the unrestricted tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee35d83-040c-40b3-90a6-270df610a21f",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "\n",
    "Ridge regression introduces L2 regularization, penalizing large coefficients and reducing model variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5d3fdbe-b554-45fd-90cc-510ad7cb9d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7196757706930821),\n",
       " np.float64(0.7455222779992702),\n",
       " 0.6125511245209703,\n",
       " 0.5758549611440126)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = ridge.predict(X_train)\n",
    "y_test_pred = ridge.predict(X_test)\n",
    "\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "rmse_train, rmse_test, r2_train, r2_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9de849-5300-46bf-97b6-9875a17ea826",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge Regression introduces L2 regularization, which penalizes large\n",
    "coefficients and stabilizes model behavior.\n",
    "\n",
    "Although its performance is slightly lower than more complex models,\n",
    "it demonstrates consistent generalization and robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8a4e35-d11a-4741-a54d-17b6520dfece",
   "metadata": {},
   "source": [
    "## Generalization Trade-off\n",
    "\n",
    "The experiments demonstrate how increasing model complexity improves\n",
    "training performance at the cost of generalization.\n",
    "\n",
    "Regularization techniques, such as limiting tree depth or applying L2\n",
    "penalties, help balance bias and variance, leading to more reliable\n",
    "models in real-world scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
